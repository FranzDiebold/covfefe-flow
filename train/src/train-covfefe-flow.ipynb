{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train covfefe-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colaboratory preparations\n",
    "With [Google Colaboratory](https://colab.research.google.com), Google's free cloud service for AI developers, we can train our machine learning (ML) models on Google's Tesla K80 GPU for free.\n",
    "\n",
    "(source: [https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d))\n",
    "\n",
    "### Folder structure within Google Drive\n",
    "```\n",
    "|-- Google Drive\n",
    "    |-- covfefe-flow\n",
    "        |-- train-covfefe-flow.ipynb\n",
    "        |-- data\n",
    "            |-- tweets.txt\n",
    "```\n",
    "\n",
    "### Enable GPU\n",
    "In the `train-covfefe-flow.ipynb` notebook click `Runtime` > `Change runtime type` > Choose `Runtime type`: `Python 3` and `Hardware accelerator`: `GPU`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Google Colab!\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for Google Colab!\n",
    "import os\n",
    "\n",
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive\n",
    "!pip install lxml beautifulsoup4 pandas seaborn keras\n",
    "\n",
    "os.chdir(\"drive/covfefe-flow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "maxlen = 35\n",
    "END_OF_TWEET = '\\n'\n",
    "TWEET_MAX_LEN = 280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    return pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def get_vocabulary_and_dictionaries():\n",
    "    printable_chars = [char for char in string.printable if char not in ('\\t', '\\r', '\\x0b', '\\x0c')]\n",
    "    extra_chars = ['‚úÖ', 'üèÜ', 'üìà', 'üìâ', 'üé•', 'üí∞', 'üì∏', '‚Ä¶']\n",
    "    vocabulary = sorted(printable_chars + extra_chars)\n",
    "    char_to_id = dict((char, i + 1) for i, char in enumerate(vocabulary))\n",
    "    char_to_id[''] = 0\n",
    "    id_to_char = dict((char_to_id[char], char) for char in char_to_id)\n",
    "    vocabulary_size = len(char_to_id)\n",
    "    return vocabulary, char_to_id, id_to_char, vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and data preparation\n",
    "\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# suppress \"looks like a URL\" UserWarnings in Beautiful Soup\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "\n",
    "def clean_tweet(original_tweet, vocabulary):\n",
    "    tweet = original_tweet\\\n",
    "            .strip()\\\n",
    "            .replace('‚Äú', '\"')\\\n",
    "            .replace('‚Äù', '\"')\\\n",
    "            .replace('‚Äô', '\\'')\\\n",
    "            .replace('‚Äò', '\\'')\\\n",
    "            .replace('‚Äî', '-')\\\n",
    "            .replace('‚Äì', '-')\n",
    "    bs_tweet = BeautifulSoup(tweet, 'lxml')\n",
    "    tweet = bs_tweet.get_text()\n",
    "    tweet += END_OF_TWEET\n",
    "    return ''.join(list(filter(lambda char: char in vocabulary, tweet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocabulary, char_to_id, id_to_char, vocabulary_size = get_vocabulary_and_dictionaries()\n",
    "print('Vocabulary size: ', vocabulary_size)\n",
    "\n",
    "\n",
    "# Input tweets\n",
    "print('Load tweets...')\n",
    "original_tweets = read_file('data/tweets.txt')\n",
    "cleaned_tweets = list(map(lambda tweet: clean_tweet(tweet, vocabulary), original_tweets))\n",
    "tweets = cleaned_tweets\n",
    "print('#Tweets: ', len(tweets))\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cleaned_tweets_lengths = list(map(len, cleaned_tweets))\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.distplot(cleaned_tweets_lengths, kde=False, bins=100, axlabel='Tweet length')\n",
    "plt.show()\n",
    "\n",
    "short_cleaned_tweets_lengths = list(filter(lambda tweet_length: tweet_length < 50, cleaned_tweets_lengths))\n",
    "plt.figure(figsize=(18, 6))\n",
    "sns.distplot(short_cleaned_tweets_lengths, kde=False, hist_kws={'cumulative': True}, axlabel='Tweet length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing characters (not in vocabulary)\n",
    "missing_chars = {}\n",
    "for original_tweet in original_tweets:\n",
    "    for char in original_tweet:\n",
    "        if char not in vocabulary:\n",
    "            if char not in missing_chars:\n",
    "                missing_chars[char] = 0\n",
    "            missing_chars[char] += 1\n",
    "for missing_char, frequency in sorted(missing_chars.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(missing_char, ':', frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.regularizers import l2  # , l1, l1_l2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "step_size = 3\n",
    "\n",
    "lstm_size = 96\n",
    "initial_learning_rate = 0.003\n",
    "regularizer = l2\n",
    "regularizer_penalty = 0.01\n",
    "# dropout = 0.2\n",
    "\n",
    "\n",
    "# Build the LSTM model\n",
    "print('Build LSTM model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(lstm_size,\n",
    "               input_shape=(maxlen, vocabulary_size),\n",
    "               # kernel_regularizer=regularizer(regularizer_penalty),\n",
    "               # recurrent_regularizer=regularizer(regularizer_penalty),\n",
    "               # bias_regularizer=regularizer(regularizer_penalty),\n",
    "               # activity_regularizer=regularizer(regularizer_penalty),\n",
    "               # dropout=dropout,\n",
    "               # recurrent_dropout=dropout,\n",
    "               return_sequences=True\n",
    "               )\n",
    "          )\n",
    "model.add(LSTM(lstm_size,\n",
    "               # kernel_regularizer=regularizer(regularizer_penalty),\n",
    "               # recurrent_regularizer=regularizer(regularizer_penalty),\n",
    "               # bias_regularizer=regularizer(regularizer_penalty),\n",
    "               # activity_regularizer=regularizer(regularizer_penalty),\n",
    "               # dropout=dropout,\n",
    "               # recurrent_dropout=dropout,\n",
    "               return_sequences=True\n",
    "               )\n",
    "          )\n",
    "model.add(LSTM(lstm_size,\n",
    "               kernel_regularizer=regularizer(regularizer_penalty),\n",
    "               # recurrent_regularizer=regularizer(regularizer_penalty),\n",
    "               # bias_regularizer=regularizer(regularizer_penalty),\n",
    "               # activity_regularizer=regularizer(regularizer_penalty),\n",
    "               # dropout=dropout,\n",
    "               # recurrent_dropout=dropout\n",
    "               )\n",
    "          )\n",
    "model.add(Dense(vocabulary_size,\n",
    "                kernel_regularizer=regularizer(regularizer_penalty),\n",
    "                bias_regularizer=regularizer(regularizer_penalty)\n",
    "                )\n",
    "          )\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# RMSprop is recommended for RNNs\n",
    "optimizer = RMSprop(lr=initial_learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODELS_FOLDER = 'models'\n",
    "MODEL_NAME = 'covfefe-flow'\n",
    "model_checkpoint_folder = '{models_folder}/{model_name}_checkpoints/'.format(\n",
    "    models_folder=MODELS_FOLDER,\n",
    "    model_name=MODEL_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, LambdaCallback, TensorBoard\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 60\n",
    "validation_split = 0.05\n",
    "\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for tweet in tweets:\n",
    "    # sentences shorter than 'maxlen'\n",
    "    for l in range(1, min(maxlen, len(tweet))):\n",
    "        sentences.append(tweet[:l])\n",
    "        next_chars.append(tweet[l])\n",
    "\n",
    "    # sentences longer than 'maxlen'\n",
    "    for j in range(0, len(tweet) - maxlen, step_size):\n",
    "        sentences.append(tweet[j: j + maxlen])\n",
    "        next_chars.append(tweet[j + maxlen])\n",
    "print('#Sentences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, vocabulary_size), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), vocabulary_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_to_id[char]] = 1\n",
    "    y[i, char_to_id[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# helper function to sample an index from a probability array\n",
    "def sample(input_predictions, temperature=1.0):\n",
    "    predictions = np.asarray(input_predictions).astype('float64')\n",
    "    exp_predictions = np.exp(np.log(predictions) / temperature)\n",
    "    normalized_predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    probabilities = np.random.multinomial(1, normalized_predictions, 1)\n",
    "    return np.argmax(probabilities)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    print()\n",
    "    print('----- Generating tweet after Epoch: %d' % epoch)\n",
    "\n",
    "    random_tweet_index = random.randint(0, len(tweets) - 1)\n",
    "    random_tweet = tweets[random_tweet_index]\n",
    "    if len(random_tweet) > maxlen:\n",
    "        start_index = random.randint(0, len(random_tweet) - maxlen - 1)\n",
    "        sentence_seed_len = maxlen\n",
    "    else:\n",
    "        start_index = 0\n",
    "        sentence_seed_len = random.randint(1, len(random_tweet) - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = random_tweet[start_index: start_index + sentence_seed_len]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(TWEET_MAX_LEN - sentence_seed_len):\n",
    "            x_pred = np.zeros((1, maxlen, vocabulary_size))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_to_id[char]] = 1.0\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = id_to_char[next_index]\n",
    "\n",
    "            if next_char == END_OF_TWEET:\n",
    "                break\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr_callback = ReduceLROnPlateau(factor=0.2, patience=1, verbose=1)\n",
    "model_checkpoint_path = model_checkpoint_folder + 'model.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "if not os.path.exists(model_checkpoint_folder):\n",
    "    os.mkdir(model_checkpoint_folder)\n",
    "model_checkpoint_callback = ModelCheckpoint(model_checkpoint_path, verbose=1, save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "tensorboard_callback = TensorBoard(\n",
    "    write_grads=True,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# Train!\n",
    "model.fit(x, y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epochs,\n",
    "          callbacks=[reduce_lr_callback, model_checkpoint_callback, print_callback, tensorboard_callback],\n",
    "          validation_split=validation_split)\n",
    "\n",
    "print('Training done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for re-import\n",
    "model.save('{models_folder}/model.h5'.format(models_folder=MODELS_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from checkpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "model_checkpoint_name = '<MODEL_CHECKPOINT_NAME_HERE>'\n",
    "model = load_model('{model_checkpoint_folder}/{model_checkpoint_name}'.format(\n",
    "    model_checkpoint_folder=model_checkpoint_folder,\n",
    "    model_checkpoint_name=model_checkpoint_name\n",
    "))\n",
    "print('Loaded model \"{model_name}\".'.format(model_name=model.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model for TensorFlow Serving\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\n",
    "from keras import backend as K\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "\n",
    "export_base_path = MODELS_FOLDER\n",
    "model_folder = MODEL_NAME\n",
    "model_version = 1\n",
    "\n",
    "export_path = os.path.join(export_base_path, model_folder, str(model_version))\n",
    "# remove model folder if it already exists\n",
    "if os.path.exists(export_path) and os.path.isdir(export_path):\n",
    "    shutil.rmtree(export_path)\n",
    "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "\n",
    "signature = predict_signature_def(inputs={\"inputs\": model.input},\n",
    "                                  outputs={\"outputs\": model.output})\n",
    "\n",
    "print('Input:', model.input)\n",
    "print('Output:', model.output)\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                         tags=[tag_constants.SERVING],\n",
    "                                         signature_def_map={\n",
    "                                            signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature\n",
    "                                         })\n",
    "    builder.save()\n",
    "\n",
    "print('Model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
